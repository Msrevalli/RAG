{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class CustomDocumentLoader(BaseLoader):\n",
    "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path to the file to load.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "        When you're implementing lazy load methods, you should use a generator\n",
    "        to yield documents one by one.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "\n",
    "    # alazy_load is OPTIONAL.\n",
    "    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
    "    async def alazy_load(\n",
    "        self,\n",
    "    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
    "        # Requires aiofiles (install with pip)\n",
    "        # https://github.com/Tinche/aiofiles\n",
    "        import aiofiles\n",
    "\n",
    "        async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            async for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loader = CustomDocumentLoader(r\"C:\\Users\\DELL\\Documents\\RAG\\Images\\image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Test out the async implementation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m loader\u001b[38;5;241m.\u001b[39malazy_load():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(doc))\n",
      "Cell \u001b[1;32mIn[46], line 45\u001b[0m, in \u001b[0;36mCustomDocumentLoader.alazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiofiles\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     44\u001b[0m     line_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m Document(\n\u001b[0;32m     47\u001b[0m             page_content\u001b[38;5;241m=\u001b[39mline,\n\u001b[0;32m     48\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: line_number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path},\n\u001b[0;32m     49\u001b[0m         )\n\u001b[0;32m     50\u001b[0m         line_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\rag\\lib\\site-packages\\aiofiles\\base.py:26\u001b[0m, in \u001b[0;36mAsyncBase.__anext__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simulate normal file iteration.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m line\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\rag\\lib\\site-packages\\aiofiles\\threadpool\\utils.py:43\u001b[0m, in \u001b[0;36m_make_delegate_method.<locals>.method\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmethod\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     42\u001b[0m     cb \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, attr_name), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor, cb)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\rag\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\rag\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "## Test out the async implementation\n",
    "async for doc in loader.alazy_load():\n",
    "    print()\n",
    "    print(type(doc))\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredFileLoader, YoutubeLoader, WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Local Files:\n",
    "   - Text files (.txt)\n",
    "   - PDF files (.pdf)\n",
    "   - Microsoft Word documents (.doc, .docx)\n",
    "   - Microsoft Excel spreadsheets (.xls, .xlsx)\n",
    "   - Microsoft PowerPoint presentations (.ppt, .pptx)\n",
    "   - Rich Text Format files (.rtf)\n",
    "   - CSV files (.csv)\n",
    "   - JSON files (.json)\n",
    "   - XML files (.xml)\n",
    "   - Markdown files (.md)\n",
    "   - HTML files (.html, .htm)\n",
    "   - Images (.jpg, .jpeg, .png, .gif, .bmp)\n",
    "   - Email files (.eml, .msg)\n",
    "\n",
    "2. Local Directories:\n",
    "   - Any directory containing one or more of the above file types\n",
    "\n",
    "3. Web URLs:\n",
    "   - Any valid website URL (e.g., https://www.example.com)\n",
    "\n",
    "The script uses different loaders based on the input type:\n",
    "\n",
    "- For local files and directories, it uses UnstructuredFileLoader, which can handle a wide variety of file formats.\n",
    "- For web URLs, it uses WebBaseLoader, which can fetch and process web content.\n",
    "\n",
    "It's important to note that while the UnstructuredFileLoader can theoretically handle many file types, the actual ability to process these files depends on the installed dependencies and the specific implementation of the UnstructuredFileLoader in your LangChain version.\n",
    "\n",
    "For optimal performance and compatibility, ensure you have the necessary dependencies installed for the file types you intend to process. Some file types may require additional libraries or tools for proper extraction and processing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader, WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "def load_dynamic_sources(source):\n",
    "    all_documents = []\n",
    "\n",
    "    if os.path.exists(source):\n",
    "        # Local directory or file\n",
    "        if os.path.isdir(source):\n",
    "            directory_loader = DirectoryLoader(source, glob=\"**/*\", loader_cls=UnstructuredFileLoader)\n",
    "            documents = directory_loader.load()\n",
    "        else:\n",
    "            file_loader = UnstructuredFileLoader(source)\n",
    "            documents = file_loader.load()\n",
    "        all_documents.extend(documents)\n",
    "    \n",
    "    elif urlparse(source).scheme:\n",
    "        # URL (website)\n",
    "        web_loader = WebBaseLoader(source)\n",
    "        documents = web_loader.load()\n",
    "        all_documents.extend(documents)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid source provided. Must be a valid local path or URL.\")\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "# Example usage:\n",
    "# docs = load_dynamic_sources('/path/to/local/directory')\n",
    "# docs = load_dynamic_sources('/path/to/local/file.txt')\n",
    "# docs = load_dynamic_sources('https://www.example.com')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=load_dynamic_sources('https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | 🦜️🔗 LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nDocument loaders | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersDocument loadersCustom Document LoaderCSVFile DirectoryHTMLJSONMarkdownMicrosoft OfficePDFText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).RetrievalDocument loadersOn this pageDocument loadersinfoHead to Integrations for documentation on built-in document loader integrations with 3rd-party tools.Use document loaders to load data from a source as Document\\'s. A Document is a piece of text\\nand associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally\\nimplement a \"lazy load\" as well for lazily loading data into memory.Get started\\u200bThe simplest loader reads in a file as text and places it all into one document.from langchain_community.document_loaders import TextLoaderloader = TextLoader(\"./index.md\")loader.load()API Reference:TextLoader[    Document(page_content=\\'---\\\\nsidebar_position: 0\\\\n---\\\\n# Document loaders\\\\n\\\\nUse document loaders to load data from a source as `Document`\\\\\\'s. A `Document` is a piece of text\\\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\\\n\\\\nEvery document loader exposes two methods:\\\\n1. \"Load\": load documents from the configured source\\\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load documents into memory lazily\\\\n\\', metadata={\\'source\\': \\'../docs/docs/modules/data_connection/document_loaders/index.md\\'})]Help us out by providing feedback on this documentation page:PreviousRetrievalNextDocument loadersGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1cb46e73d00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(docs,embedding=embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever=RunnableLambda(vectorstore.similarity_search).bind(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what LangChain is based on the provided context. Let me start by reading through the context carefully. \n",
      "\n",
      "The context is a document from LangChain's documentation, specifically about document loaders. It mentions that document loaders are used to load data from various sources into documents. Each document consists of text and associated metadata. Examples given include loading from .txt files, web pages, and even YouTube transcripts.\n",
      "\n",
      "The document explains that each loader has a \"load\" method to load documents and an optional \"lazy load\" method for loading data into memory as needed. There's also a mention of a \"load and split\" method, which uses a text splitter. \n",
      "\n",
      "I see that there's an example provided where they import TextLoader from langchain_community.document_loaders and use it to load a file. The API reference section shows how the TextLoader is used, which gives me an idea of how these loaders are implemented.\n",
      "\n",
      "So, putting this together, LangChain seems to be a framework or library that helps in loading and processing documents from various sources. It provides different loaders for different file types and sources, making it flexible for different use cases. The emphasis is on efficiently loading and managing documents, possibly for tasks like data retrieval, processing, or analysis.\n",
      "\n",
      "I'm not entirely sure about the broader scope of LangChain beyond document loaders, but from this context, it's clear that document loaders are a key component, allowing users to handle multiple data sources and formats seamlessly.\n",
      "</think>\n",
      "\n",
      "LangChain is a framework that provides tools for loading and processing documents from various sources. It offers document loaders, which are components designed to load data into a structured format consisting of text and metadata. These loaders support multiple sources, such as text files, web pages, and YouTube transcripts. Each loader includes a \"load\" method for fetching documents and may include a \"lazy load\" feature for efficient memory usage. Additionally, there's a \"load and split\" method for dividing documents using text splitters. This functionality makes LangChain versatile for handling different data formats and sources, facilitating tasks like data retrieval and analysis.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "rag_chain={\"context\":retriever,\"question\":RunnablePassthrough()}|prompt|llm\n",
    "\n",
    "response=rag_chain.invoke(\"tell me about langchain\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
